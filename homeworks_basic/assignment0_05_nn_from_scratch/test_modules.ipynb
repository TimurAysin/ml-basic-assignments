{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: this notebook belongs to [Practical DL](https://docs.google.com/forms/d/e/1FAIpQLScvrVtuwrHSlxWqHnLt1V-_7h2eON_mlRR6MUb3xEe5x9LuoA/viewform?usp=sf_link) course by Yandex School of Data Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_BatchNormalization (__main__.TestLayers) ... FAIL\n",
      "test_ClassNLLCriterion (__main__.TestLayers) ... ok\n",
      "test_ClassNLLCriterionUnstable (__main__.TestLayers) ... ok\n",
      "test_Dropout (__main__.TestLayers) ... ok\n",
      "test_ELU (__main__.TestLayers) ... ok\n",
      "test_LeakyReLU (__main__.TestLayers) ... ok\n",
      "test_Linear (__main__.TestLayers) ... ok\n",
      "test_LogSoftMax (__main__.TestLayers) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9127199  -0.84988785 -1.7849236  -0.40178728  0.8827498   0.9236938\n",
      "  -1.3702961   0.64371806 -0.50856173  1.710391    0.9915215   0.99915284\n",
      "  -0.16526239  0.0026371  -1.5673922  -1.1120197 ]\n",
      " [ 1.2098203   1.7178233   1.6724559   1.5005399   0.02515719  0.45449796\n",
      "   1.622625    1.3146566  -1.0538198  -0.14640126 -1.322632    1.4321054\n",
      "   0.13553938 -0.7010689  -0.06004297  0.7953673 ]\n",
      " [-0.763908   -0.57036674  0.41254637  0.03420081  0.7468772   0.02937603\n",
      "   1.2736509   0.2156233   0.31807786  1.3687284  -0.40371117 -1.2906559\n",
      "  -1.2444685   1.1821837   0.17623875  0.98452044]\n",
      " [-0.70274025 -0.49154165 -1.8663201  -0.5792839   0.24013427 -0.44128194\n",
      "  -0.29083413  1.24551    -0.19586909  0.02085049  0.72297305 -0.42290443\n",
      "   0.5236954   1.4978486   1.2330991  -0.8070956 ]\n",
      " [ 1.4015014   0.5633968  -1.0244204  -0.13080566  1.5602971   0.02137395\n",
      "  -0.6934252   0.3853076  -1.1051723  -1.3160666  -1.2956529  -1.3978536\n",
      "  -1.6118854  -0.9871135  -0.24561648 -0.80720127]\n",
      " [-0.5747077   1.9850495  -0.1052378   0.49833235 -1.2349933  -0.9434842\n",
      "  -1.7803973  -0.44455615 -0.67854375 -1.2026744  -1.4247752  -1.4211894\n",
      "   0.3086981  -0.7986101  -0.8355334   0.3248672 ]\n",
      " [ 0.7027229  -1.0083553   1.0172087   0.40730494 -1.4977092   0.7969617\n",
      "   1.5314674  -0.7535424  -0.49746776  1.2748942   0.6668418  -1.1143872\n",
      "  -1.3816082  -0.16575274 -0.7874351   0.6244633 ]\n",
      " [-0.6287443   0.5585576   0.66228473 -1.553761   -0.984592    0.8084237\n",
      "   1.4346353   0.52780557 -0.12237302 -1.3740177  -0.36845958  0.06557427\n",
      "  -0.74446785 -0.58327365 -0.33139423 -1.1733193 ]\n",
      " [-0.5729605  -0.3427342  -0.65109855 -1.2078454   1.17479     0.48266152\n",
      "   0.6235412   1.1737614  -0.08135846 -1.429055    0.02703374  0.12471627\n",
      "   0.980713    0.02467089 -1.9796515  -0.35524052]\n",
      " [-0.10205889  1.181288    0.1315172  -0.45330808  1.4888817   0.48994896\n",
      "  -1.0381202  -1.1130337  -1.1454126  -0.5563319  -1.258242   -1.1826833\n",
      "  -0.09254002 -0.8516168   1.4913056  -1.0456138 ]\n",
      " [ 0.00975236  0.38020617  1.5563338  -1.2286136  -0.41167855  1.0987346\n",
      "   1.326044    0.9555723  -0.8526597  -0.974786    0.3191509   1.3621033\n",
      "   0.20174709  0.49863756 -1.5534488   1.1725295 ]\n",
      " [-0.9312818   0.14285375 -0.29097646 -0.01034545 -1.0278273  -1.0118546\n",
      "   0.36096242 -0.4842329   0.25269398 -0.9736682  -0.20402944  0.01428125\n",
      "  -1.762747   -0.2615357  -1.2150851  -1.2069718 ]\n",
      " [ 1.5468646   0.02136367  0.99857724 -0.85610473  0.547519    0.9484082\n",
      "   0.30546877  0.303845    0.12685585 -0.5427512   1.2587359   1.0113761\n",
      "   1.4935461  -1.0079074   0.77668434  1.8674936 ]\n",
      " [-0.790546   -0.8129517   0.68114585  0.16274482  0.99659616 -1.1209339\n",
      "   1.0613384  -0.70129174 -1.3224581  -1.0120367   0.8887438   0.44201934\n",
      "   0.563948   -0.18356906  0.97008085  0.00846013]\n",
      " [ 1.2495921   0.54495424 -0.518027   -0.21957469 -0.76202106  0.8286369\n",
      "  -0.04163472 -0.4514321   1.5387512  -0.39700967  0.01507584  1.3237277\n",
      "   0.17457066 -1.0978526   0.9735983   0.65363145]\n",
      " [-0.84431106 -0.49232545  0.93864334  0.38454387  0.05524036  0.9593723\n",
      "   1.0297453  -0.44787386 -0.8791724  -0.8574849   0.6051377  -1.7317439\n",
      "   0.3842907   1.2246116   1.2025567  -0.29123968]\n",
      " [ 0.9449843  -0.6577806   1.1833583  -1.2439935  -0.39323038  0.40877035\n",
      "  -1.3690759  -0.27079064  0.6922211   1.0822971   0.32117003  0.59005576\n",
      "   0.24363565 -0.64350075 -0.51488286 -0.8089227 ]\n",
      " [ 1.0792776   0.94815385 -0.45956188 -0.2102393   1.4061749  -1.0879395\n",
      "   0.26992914  0.5625992   0.63186014 -1.6803476   0.97278535  1.3601665\n",
      "  -0.04115231  0.95874894  1.3599385   1.04653   ]\n",
      " [-1.1897522   0.8297621   0.36272177 -0.33279377  0.6611191   0.8206917\n",
      "   1.5490133   0.32266432 -1.0581421   2.0367181   0.8665744  -1.2872118\n",
      "   2.0755384   1.5581722  -0.06651942  0.57547635]\n",
      " [-0.6480768  -0.8594451  -0.6473072   0.8662937  -1.8043987  -0.76187307\n",
      "  -0.43119547  0.11135344  1.2168492  -0.24913578 -0.61018884  0.7292776\n",
      "  -0.28660417 -0.66220117 -0.79990745 -0.88083863]\n",
      " [-1.1985778   0.6863939  -0.34612     1.2003611  -0.5433929   0.10226019\n",
      "   0.44475827 -1.2044284   0.89507437 -1.0926313   1.3402431  -1.1995401\n",
      "  -0.5173504  -1.1381257   1.0228815   0.2548928 ]\n",
      " [ 0.19338375 -0.8255562   0.8474175  -0.9686923   1.2178967  -1.3910143\n",
      "  -1.2149388  -1.1477462  -0.10304327  0.4755884  -1.5841593   0.863989\n",
      "   0.37271413  0.34954634 -0.0938676  -0.04196595]\n",
      " [ 0.45807004 -0.60290295 -1.2535541   1.1463485   1.3938166  -0.19999632\n",
      "  -0.91501355  0.6582313  -0.3539653  -1.6160592  -1.2331831   0.5659455\n",
      "   0.8347444  -1.4502627  -0.8701428  -0.51209223]\n",
      " [ 0.89515126 -1.060671   -1.3819822   0.9427024  -1.1670586  -0.09797685\n",
      "  -1.0352312  -0.6786079  -0.4289023   1.1433947   0.9452006   1.0928764\n",
      "   0.25456077  0.83925056 -0.96963525 -0.4134196 ]\n",
      " [ 1.1169529  -1.0481552  -1.4830288  -1.0067368  -1.6604693  -0.7485468\n",
      "   0.26134852 -0.01606816  1.2754633   1.2855624  -0.59597325 -0.96735835\n",
      "  -0.2018271   0.5518002  -0.8806041   0.80775833]\n",
      " [-0.5618015   0.83972484 -0.41092592 -0.6807338   1.4423951   0.5228659\n",
      "  -1.4014392   1.6541514   0.3928327   1.5635234   0.8552698  -0.42108476\n",
      "  -1.3982614  -0.56336826 -0.21334809  0.8009839 ]\n",
      " [-0.656131   -0.53945935  0.99380666  1.4399358   0.01094187 -1.5661515\n",
      "  -0.9084124  -1.4169308   1.5438452  -1.27905     0.14077638 -0.831648\n",
      "   0.03317469  0.83788407  1.276197   -0.677422  ]\n",
      " [-1.3106978  -0.6630494   1.2320398   1.1789103   0.25566933  0.2968078\n",
      "   0.57277894 -0.9313334   1.1771117  -0.18646166 -0.5379448  -0.08010494\n",
      "  -0.7607733  -0.96450794  0.9153831  -1.0388008 ]\n",
      " [ 0.65804774 -0.15635872 -0.5127525  -0.7116953  -0.5437726   0.14181402\n",
      "  -0.8150407   0.18908326  0.03491769  0.58414483  1.180667    0.76202375\n",
      "  -0.53128594 -1.4106147  -1.114745    0.5412889 ]\n",
      " [-1.3709158   0.6062684   0.13875286 -0.5433491   0.8560042  -1.8363664\n",
      "  -1.6544324   0.6918682  -0.24998623  0.8882183  -0.29371932 -0.87834907\n",
      "   1.707805    1.2603168   0.4644879  -0.4620222 ]\n",
      " [ 0.18195449  0.17609483 -1.5645679   1.2534611  -1.2782735   0.7982392\n",
      "  -0.25302908 -0.35637188 -0.08226232  1.5441973   0.5985564   0.9856714\n",
      "   0.54760593  0.9436148   0.8451884  -0.5273816 ]\n",
      " [ 0.286416   -0.20034966  1.4719933   1.3239844  -1.6528429   0.27388018\n",
      "   1.5452107  -0.53751     0.6226161   1.9074605  -1.5837867   0.50165224\n",
      "  -0.09629305  1.7409574   1.3916119   1.7033051 ]]\n",
      "[[ 1.2467036  -1.5333823  -1.607176   -0.38876852  1.0792584   1.5462251\n",
      "  -1.3520464   0.27341276 -0.41910052  1.8349206   1.0996314   1.1532466\n",
      "  -0.12077329 -0.3015273  -0.8811826  -1.4509548 ]\n",
      " [ 1.1672583   1.039947    1.7055295   1.5309231   0.19360434  0.81566703\n",
      "   1.634227    1.0107315  -0.89971185 -0.1835116  -1.1378863   1.4944464\n",
      "   0.42559013 -0.92701817  0.6702727   0.40914503]\n",
      " [-0.48280624 -1.3777456   0.6044034   0.06256986  0.9639006   0.06094265\n",
      "   1.2856643   0.15528299  0.21275242  1.463725   -0.30606785 -1.2301474\n",
      "  -1.8739202   0.8802687   0.44578472  0.6546129 ]\n",
      " [-0.95579886 -1.2980273  -1.6964028  -0.49138212  0.30159283 -0.32330164\n",
      "  -0.2463874   1.2173     -0.55779964  0.05416598  0.88737    -0.34159583\n",
      "   0.46751806  1.2847636   1.6833862  -1.1589296 ]\n",
      " [ 1.3654221  -0.02753107 -0.8503092  -0.13082905  1.6460838  -0.02258191\n",
      "  -0.6500142   0.40958467 -0.9309232  -1.5799781  -1.1452577  -1.2517939\n",
      "  -1.3840597  -1.238484    0.5597479  -1.0580465 ]\n",
      " [-0.49564606  1.3324462  -0.09342603  0.51594996 -1.0924833  -0.9353014\n",
      "  -1.6877834  -1.0045843  -0.7740991  -1.2223811  -1.2674369  -1.2742691\n",
      "  -0.15228614 -1.0275311  -0.50293165  0.01096235]\n",
      " [ 0.61133015 -1.6666847   1.0608792   0.4138496  -1.3593813   1.1297898\n",
      "   1.533959   -1.3381917  -0.8085814   1.1070238   0.76822734 -1.066552\n",
      "  -1.1546979  -0.44844308 -0.06059011  0.39967367]\n",
      " [-0.4314638  -0.12479772  0.88700634 -1.4722651  -0.852793    0.66827136\n",
      "   1.4413811   0.03485189  0.11729422 -1.4296527  -0.17263167  0.11028986\n",
      "  -0.9833193  -0.81599957 -0.48141372 -1.6941541 ]\n",
      " [-0.5694525  -0.9599998  -0.59542596 -1.2051989   1.2989997   0.29676527\n",
      "   0.662081    0.83640957 -0.00547793 -1.4970229   0.11932072  0.3066116\n",
      "   0.9418298  -0.23244025 -1.4076658  -0.7095374 ]\n",
      " [-0.47477627  0.48515168  0.23871495 -0.46327904  1.6376735   0.33253545\n",
      "  -0.9873824  -1.1861968  -1.2123483  -1.038659   -1.0780873  -1.0389583\n",
      "  -0.8825232  -1.1417989   1.5037804  -1.3605727 ]\n",
      " [ 0.07877108 -0.29847348  1.629026   -1.2375938  -0.33873412  1.4187655\n",
      "   1.3919318   0.96520627 -0.84425473 -1.2182488   0.5697274   1.4098213\n",
      "   0.21852136  0.24555129 -0.857911    0.87137675]\n",
      " [-1.0082161  -0.6230929  -0.26217902  0.02337573 -0.90707517 -1.2157067\n",
      "   0.42936113 -0.61892885  0.28440818 -1.3268533  -0.05780001  0.10701765\n",
      "  -1.7218493  -0.59269434 -1.4515073  -1.464816  ]\n",
      " [ 1.5746576  -0.5958882   1.0505439  -0.76807755  0.60907197  0.8140905\n",
      "   0.36102158 -0.08285092 -0.3360487  -0.5657107   1.3591306   1.0891783\n",
      "   1.8864458  -1.3593653   0.8651786   1.5483509 ]\n",
      " [-1.0675235  -1.4372877   0.828616    0.2306043   1.1656859  -1.0923978\n",
      "   1.1100752  -0.86770076 -1.1898551  -1.2360936   0.9863779   0.54669607\n",
      "   0.08967873 -0.51198274  1.4104561  -0.3542106 ]\n",
      " [ 1.041187   -0.21750383 -0.41579032 -0.12113489 -0.64547294  0.76035565\n",
      "   0.00991474 -0.77654576  1.3952296  -0.4312469   0.13431974  1.3609726\n",
      "   0.50147706 -1.3136549   1.7136035   0.43285504]\n",
      " [-0.5627743  -1.2667093   0.99503344  0.39015257  0.11306759  1.1804367\n",
      "   1.156355   -1.077042   -0.680197   -0.9180006   0.7566627  -1.5970346\n",
      "   0.2667398   0.93192524  1.3975251  -0.53463584]\n",
      " [ 1.038882   -1.2918187   1.2311453  -1.1486607  -0.35251456  0.9252966\n",
      "  -1.2761022  -0.8197407   0.78555447  0.81786305  0.43457142  0.642275\n",
      "   0.16329938 -0.875875   -0.59638274 -1.0599173 ]\n",
      " [ 1.3769829   0.27639183 -0.33438978 -0.12059407  1.4544638  -1.0498822\n",
      "   0.32681876  0.01788002  0.39630795 -1.7449965   1.2272433   1.421162\n",
      "   0.26193115  0.66919804  1.5662837   0.6798482 ]\n",
      " [-1.1137706   0.27552208  0.3859807  -0.24209678  0.7833825   1.3557526\n",
      "   1.5513715  -0.14575094 -1.4138486   1.7770023   1.1205057  -1.3289956\n",
      "   1.625473    1.2588282   0.07035058  0.29635546]\n",
      " [-0.34648052 -1.4756652  -0.56023604  0.9606292  -1.670394   -0.4987518\n",
      "  -0.40125886  0.11425611  1.5637699  -0.59342915 -0.47097278  0.78286165\n",
      "  -0.18557423 -0.9811572  -0.18172254 -1.1520147 ]\n",
      " [-1.0489568  -0.0054511  -0.27714938  1.370492   -0.4502834   0.24131109\n",
      "   0.48479185 -1.4084811   0.5928788  -1.1674914   1.4271283  -1.2592732\n",
      "  -0.33573255 -1.526931    2.0303133   0.00728832]\n",
      " [ 0.30874467 -1.4358835   0.8880931  -0.88765496  1.2998949  -0.91151005\n",
      "  -1.1295763  -1.3416358  -0.09750275  0.23812439 -1.4536176   0.91096485\n",
      "  -0.18395847  0.0829557  -0.23207922 -0.31989413]\n",
      " [ 0.19962698 -1.2289718  -1.0848576   1.17213     1.4491248  -0.40143922\n",
      "  -0.8377099   0.41705388 -0.3190965  -1.7151483  -1.0539854   0.75768137\n",
      "   0.6058575  -1.7432523  -1.106985   -0.8987626 ]\n",
      " [ 0.59013426 -1.696402   -1.4407159   0.9558402  -1.047088    0.4830825\n",
      "  -0.9685096  -1.2043307  -1.0019174   0.8525144   1.1212105   1.1464479\n",
      "  -0.41305062  0.5753475  -1.1635872  -0.67241603]\n",
      " [ 1.30335    -1.6479689  -1.4438391  -0.9184415  -1.562949   -1.0230855\n",
      "   0.30371022 -0.22776291  1.396096    1.1525995  -0.50389516 -0.82511836\n",
      "  -0.45941973  0.31466928 -0.884052    0.4251081 ]\n",
      " [-0.30455676  0.17251156 -0.22606589 -0.63237447  1.5043775   0.79175115\n",
      "  -1.4549942   1.1228085  -0.04778745  1.4357169   0.95931953 -0.26309142\n",
      "  -1.8585387  -0.792039    0.15451168  0.43737116]\n",
      " [-0.7852507  -1.2060822   1.1644866   1.5353283   0.08459327 -1.0332726\n",
      "  -0.82985944 -1.460418    1.4441333  -1.3812492   0.24160239 -0.74897075\n",
      "   0.21299832  0.5414093   1.2588601  -0.9437106 ]\n",
      " [-1.663475   -1.2889806   1.3768442   1.1975312   0.3223235   0.30459175\n",
      "   0.6496475  -0.9785177   1.5555682  -0.29416835 -0.36315247  0.06420274\n",
      "  -2.0161583  -1.2407299   0.90705365 -1.3509107 ]\n",
      " [ 0.3526665  -0.86541545 -0.37436995 -0.6465122  -0.48528576  0.6861382\n",
      "  -0.7394277   0.20358682 -0.08375489  0.63140464  1.4304717   0.8075795\n",
      "  -1.1294559  -1.6322742  -0.8832056   0.32418424]\n",
      " [-1.4468372  -0.01228855  0.34681988 -0.5157726   0.92324716 -1.2638721\n",
      "  -1.7044778   0.694247   -0.0164461   0.682905   -0.21066856 -0.8918145\n",
      "   1.2372841   1.106771    0.7214404  -0.73292476]\n",
      " [ 0.29177818 -0.4614755  -1.3903542   1.2831855  -1.2197875   1.4334095\n",
      "  -0.2011015  -0.97023124  0.15280423  1.347704    0.71192026  1.0270333\n",
      "   0.61499876  0.6543495   1.308318   -0.80349946]\n",
      " [-0.03621588 -0.99670947  1.8156816   1.3838695  -1.4979258   0.62205505\n",
      "   1.619492   -0.93543094  0.25225708  1.6479958  -1.4550688   0.6943347\n",
      "  -0.8217966   1.4505649   1.7518735   1.5027125 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_Sequential (__main__.TestLayers) ... FAIL\n",
      "test_SoftMax (__main__.TestLayers) ... ok\n",
      "test_SoftPlus (__main__.TestLayers) ... ok\n",
      "test_adam_optimizer (__main__.TestLayers) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_BatchNormalization (__main__.TestLayers)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_25250/4108405182.py\", line 124, in test_BatchNormalization\n",
      "    self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n",
      "AssertionError: False is not true\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_Sequential (__main__.TestLayers)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_25250/4108405182.py\", line 174, in test_Sequential\n",
      "    self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))\n",
      "AssertionError: False is not true\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.284s\n",
      "\n",
      "FAILED (failures=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=12 errors=0 failures=2>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestLayers(unittest.TestCase):\n",
    "    def test_Linear(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in, n_out = 2, 3, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Linear(n_in, n_out)\n",
    "            custom_layer = Linear(n_in, n_out)\n",
    "            custom_layer.W = torch_layer.weight.data.numpy()\n",
    "            custom_layer.b = torch_layer.bias.data.numpy()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_out)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "        \n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "            # 3. check layer parameters grad\n",
    "            custom_layer.accGradParameters(layer_input, next_layer_grad)\n",
    "            weight_grad = custom_layer.gradW\n",
    "            bias_grad = custom_layer.gradb\n",
    "            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))\n",
    "            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n",
    "\n",
    "    def test_SoftMax(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Softmax(dim=1)\n",
    "            custom_layer = SoftMax()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n",
    "            next_layer_grad = next_layer_grad.clip(1e-5,1.)\n",
    "            next_layer_grad = 1. / next_layer_grad\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-5))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n",
    "            \n",
    "    def test_LogSoftMax(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.LogSoftmax(dim=1)\n",
    "            custom_layer = LogSoftMax()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_BatchNormalization(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 32, 16\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            slope = np.random.uniform(0.01, 0.05)\n",
    "            alpha = 0.9\n",
    "            custom_layer = BatchNormalization(alpha)\n",
    "            custom_layer.train()\n",
    "            torch_layer = torch.nn.BatchNorm1d(n_in, eps=custom_layer.EPS, momentum=1.-alpha, affine=False)\n",
    "            custom_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n",
    "            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            # please, don't increase `atol` parameter, it's garanteed that you can implement batch norm layer\n",
    "            # with tolerance 1e-5\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n",
    "\n",
    "            # 3. check moving mean\n",
    "            self.assertTrue(np.allclose(custom_layer.moving_mean, torch_layer.running_mean.numpy()))\n",
    "            # we don't check moving_variance because pytorch uses slightly different formula for it:\n",
    "            # it computes moving average for unbiased variance (i.e var*N/(N-1))\n",
    "            #self.assertTrue(np.allclose(custom_layer.moving_variance, torch_layer.running_var.numpy()))\n",
    "\n",
    "            # 4. check evaluation mode\n",
    "            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "            custom_layer.evaluate()\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            torch_layer.eval()\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "            \n",
    "    def test_Sequential(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            alpha = 0.9\n",
    "            torch_layer = torch.nn.BatchNorm1d(n_in, eps=BatchNormalization.EPS, momentum=1.-alpha, affine=True)\n",
    "            torch_layer.bias.data = torch.from_numpy(np.random.random(n_in).astype(np.float32))\n",
    "            custom_layer = Sequential()\n",
    "            bn_layer = BatchNormalization(alpha)\n",
    "            bn_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n",
    "            bn_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "            custom_layer.add(bn_layer)\n",
    "            scaling_layer = ChannelwiseScaling(n_in)\n",
    "            scaling_layer.gamma = torch_layer.weight.data.numpy()\n",
    "            scaling_layer.beta = torch_layer.bias.data.numpy()\n",
    "            custom_layer.add(scaling_layer)\n",
    "            custom_layer.train()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)            \n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))\n",
    "\n",
    "            # 3. check layer parameters grad\n",
    "            weight_grad, bias_grad = custom_layer.getGradParameters()[1]\n",
    "            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-3))\n",
    "            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-3))\n",
    "\n",
    "    def test_Dropout(self):\n",
    "        np.random.seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            p = np.random.uniform(0.3, 0.7)\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.all(np.logical_or(np.isclose(layer_output, 0), \n",
    "                                        np.isclose(layer_output*(1.-p), layer_input))))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            self.assertTrue(np.all(np.logical_or(np.isclose(layer_grad, 0), \n",
    "                                        np.isclose(layer_grad*(1.-p), next_layer_grad))))\n",
    "\n",
    "            # 3. check evaluation mode\n",
    "            layer.evaluate()\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.allclose(layer_output, layer_input))\n",
    "\n",
    "            # 4. check mask\n",
    "            p = 0.0\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.allclose(layer_output, layer_input))\n",
    "\n",
    "            p = 0.5\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "            layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            zeroed_elem_mask = np.isclose(layer_output, 0)\n",
    "            layer_grad = layer.updateGradInput(layer_input, next_layer_grad)        \n",
    "            self.assertTrue(np.all(zeroed_elem_mask == np.isclose(layer_grad, 0)))\n",
    "\n",
    "            # 5. dropout mask should be generated independently for every input matrix element, not for row/column\n",
    "            batch_size, n_in = 1000, 1\n",
    "            p = 0.8\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "\n",
    "            layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n",
    "\n",
    "            layer_input = layer_input.T\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n",
    "    def test_LeakyReLU(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            slope = np.random.uniform(0.01, 0.05)\n",
    "            torch_layer = torch.nn.LeakyReLU(slope)\n",
    "            custom_layer = LeakyReLU(slope)\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_ELU(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            alpha = 1.0\n",
    "            torch_layer = torch.nn.ELU(alpha)\n",
    "            custom_layer = ELU(alpha)\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_SoftPlus(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Softplus()\n",
    "            custom_layer = SoftPlus()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_ClassNLLCriterionUnstable(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.NLLLoss()\n",
    "            custom_layer = ClassNLLCriterionUnstable()\n",
    "\n",
    "            layer_input = np.random.uniform(0, 1, (batch_size, n_in)).astype(np.float32)\n",
    "            layer_input /= layer_input.sum(axis=-1, keepdims=True)\n",
    "            layer_input = layer_input.clip(custom_layer.EPS, 1. - custom_layer.EPS)  # unifies input\n",
    "            target_labels = np.random.choice(n_in, batch_size)\n",
    "            target = np.zeros((batch_size, n_in), np.float32)\n",
    "            target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input, target)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(\n",
    "                torch.log(layer_input_var),\n",
    "                torch.from_numpy(target_labels))\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n",
    "            torch_layer_output_var.backward()\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "    def test_ClassNLLCriterion(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.NLLLoss()\n",
    "            custom_layer = ClassNLLCriterion()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            layer_input = torch.nn.LogSoftmax(dim=1)(torch.from_numpy(layer_input)).data.numpy()\n",
    "            target_labels = np.random.choice(n_in, batch_size)\n",
    "            target = np.zeros((batch_size, n_in), np.float32)\n",
    "            target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input, target)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(\n",
    "                layer_input_var,\n",
    "                torch.from_numpy(target_labels)\n",
    "            )\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n",
    "            torch_layer_output_var.backward()\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "            \n",
    "    def test_adam_optimizer(self):\n",
    "        state = {}  \n",
    "        config = {'learning_rate': 1e-3, 'beta1': 0.9, 'beta2':0.999, 'epsilon':1e-8}\n",
    "        variables = [[np.arange(10).astype(np.float64)]]\n",
    "        gradients = [[np.arange(10).astype(np.float64)]]\n",
    "        adam_optimizer(variables, gradients, config, state)\n",
    "        self.assertTrue(np.allclose(state['m'][0], np.array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, \n",
    "                                                             0.6, 0.7, 0.8, 0.9])))\n",
    "        self.assertTrue(np.allclose(state['v'][0], np.array([0., 0.001, 0.004, 0.009, 0.016, 0.025, \n",
    "                                                             0.036, 0.049, 0.064, 0.081])))\n",
    "        self.assertTrue(state['t'] == 1)\n",
    "        self.assertTrue(np.allclose(variables[0][0], np.array([0., 0.999, 1.999, 2.999, 3.999, 4.999, \n",
    "                                                               5.999, 6.999, 7.999, 8.999])))\n",
    "        adam_optimizer(variables, gradients, config, state)\n",
    "        self.assertTrue(np.allclose(state['m'][0], np.array([0., 0.19, 0.38, 0.57, 0.76, 0.95, 1.14, \n",
    "                                                             1.33, 1.52, 1.71])))\n",
    "        self.assertTrue(np.allclose(state['v'][0], np.array([0., 0.001999, 0.007996, 0.017991, \n",
    "                                                             0.031984, 0.049975, 0.071964, 0.097951, \n",
    "                                                             0.127936, 0.161919])))\n",
    "        self.assertTrue(state['t'] == 2)\n",
    "        self.assertTrue(np.allclose(variables[0][0], np.array([0., 0.998, 1.998, 2.998, 3.998, 4.998, \n",
    "                                                               5.998, 6.998, 7.998, 8.998])))\n",
    "    \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestLayers)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_Conv2d (__main__.TestAdvancedLayers) ... ERROR\n",
      "test_MaxPool2d (__main__.TestAdvancedLayers) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_Conv2d (__main__.TestAdvancedLayers)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_25250/3182513238.py\", line 23, in test_Conv2d\n",
      "    self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
      "  File \"<__array_function__ internals>\", line 5, in allclose\n",
      "  File \"/home/taysin/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py\", line 2249, in allclose\n",
      "    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n",
      "  File \"<__array_function__ internals>\", line 5, in isclose\n",
      "  File \"/home/taysin/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py\", line 2356, in isclose\n",
      "    yfin = isfinite(y)\n",
      "TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_MaxPool2d (__main__.TestAdvancedLayers)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_25250/3182513238.py\", line 61, in test_MaxPool2d\n",
      "    self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
      "  File \"<__array_function__ internals>\", line 5, in allclose\n",
      "  File \"/home/taysin/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py\", line 2249, in allclose\n",
      "    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n",
      "  File \"<__array_function__ internals>\", line 5, in isclose\n",
      "  File \"/home/taysin/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py\", line 2356, in isclose\n",
      "    yfin = isfinite(y)\n",
      "TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.017s\n",
      "\n",
      "FAILED (errors=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=2 failures=0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestAdvancedLayers(unittest.TestCase):\n",
    "    def test_Conv2d(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in, n_out = 2, 3, 4\n",
    "        h,w = 5,6\n",
    "        kern_size = 3\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Conv2d(n_in, n_out, kern_size, padding=1)\n",
    "            custom_layer = Conv2d(n_in, n_out, kern_size)\n",
    "            custom_layer.W = torch_layer.weight.data.numpy() # [n_out, n_in, kern, kern]\n",
    "            custom_layer.b = torch_layer.bias.data.numpy()\n",
    "\n",
    "            layer_input = np.random.uniform(-1, 1, (batch_size, n_in, h,w)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-1, 1, (batch_size, n_out, h, w)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "        \n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "            \n",
    "            # 3. check layer parameters grad\n",
    "            custom_layer.accGradParameters(layer_input, next_layer_grad)\n",
    "            weight_grad = custom_layer.gradW\n",
    "            bias_grad = custom_layer.gradb\n",
    "            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "            #m = ~np.isclose(torch_weight_grad, weight_grad, atol=1e-5)\n",
    "            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6, ))\n",
    "            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n",
    "            \n",
    "    def test_MaxPool2d(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 3\n",
    "        h,w = 4,6\n",
    "        kern_size = 2\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.MaxPool2d(kern_size)\n",
    "            custom_layer = MaxPool2d(kern_size)\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in, h,w)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_in, \n",
    "                                                          h // kern_size, w // kern_size)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n",
    "        \n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestAdvancedLayers)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ed34618cfd61fdfc328074d59e26af48c4a0365e1cbfa0189d879359feb3f5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
